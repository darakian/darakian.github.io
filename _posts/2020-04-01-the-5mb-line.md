---
layout: post
title:  "The one line that allocated 5 megabytes of memory"
date:   2020-04-01 18:00:00 -0700
---
# Preamble
Recently at work our primary piece of web code was hit with a memory leak. We all panicked of course, but then we all took a hard look at what was going on. This blog post doesn't cover that memory leak, but covers another memory hog I found while digging into things.

# The Line
Now, we have a nice little web server built on the django web framework and in it we have a utilities module with a number of fairly general functions; nothing super uncommon. So imagine my surprise when I rig up our webserver with [tracemalloc](https://docs.python.org/3/library/tracemalloc.html), boot the server then immediately take the server down and I see
```
...
utils/__init__.py:48: size=5412 KiB, count=65307, average=85 B
...
```
What is line 48? Well
```
_all_chars = [chr(i) for i in range(65535)]
```
Massive confusion here for a number of reasons: Why are we generating a list of all characters? why is this list not actually all characters? How is this taking up 5 megabytes of memory? The last question is stumped me for a bit until I read through the [python import reference](https://docs.python.org/3/reference/import.html#regular-packages). Anything in the module level score of an `__init__.py` file will get executed whenever the module is imported. Well... `_all_chars` is at the module level and we do import utils all over the place. So, we're creating tens of thousands of copies of the same list of characters. Why not make this thing global and be done with it?

# What's actually going on
Let's take a look at the full code sample and consider what's actually happening here
```
_all_chars = [chr(i) for i in range(65535)]
_space_chars = [c for c in _all_chars if unicodedata.category(c) == 'Zs']
_line_break_chars = [c for c in _all_chars
                     if unicodedata.category(c) in ('Zp', 'Zl')]

_space_char_regex = re.compile('[%s]' % re.escape(''.join(_space_chars)))
_line_break_char_regex = re.compile(
    '[%s]' % re.escape(''.join(_line_break_chars))
)
_null_char_regex = re.compile(re.escape(chr(0)))


def clean_string(value):
    return _line_break_char_regex.sub(
        '\n', _space_char_regex.sub(
            ' ', _null_char_regex.sub('', value)
        )
    ).strip()
```
If your eyes glaze over while reading this code I don't blame you. It certainly took me a while to wrap my head around it. Let's break it down though.
First define some character lists
```
_all_chars = [chr(i) for i in range(65535)]
_space_chars = [c for c in _all_chars if unicodedata.category(c) == 'Zs']
_line_break_chars = [c for c in _all_chars
                     if unicodedata.category(c) in ('Zp', 'Zl')]
```

It might seem a bit odd to define these lists, but lets circle back to this later. On to the next section
```
_space_char_regex = re.compile('[%s]' % re.escape(''.join(_space_chars)))
_line_break_char_regex = re.compile(
    '[%s]' % re.escape(''.join(_line_break_chars))
)
_null_char_regex = re.compile(re.escape(chr(0)))
```
This is a bit more straight forward. It's just defining some regular expressions based on the above character sets and on the null character.

And onto the main act
```
def clean_string(value):
    return _line_break_char_regex.sub(
        '\n', _space_char_regex.sub(
            ' ', _null_char_regex.sub('', value)
        )
    ).strip()
```
As you might expect given the run up; this function uses the regular expressions above to manipulate a string. Curiously though the only use of the regular expressions is to do substitutions. The original author reinvented `str.replace()` for some reason. I'll skip the code from my rewrite, but it's a simple chained `str.replace()`. The rewrite had the effect of lowering system memory consumption by ~5.5MB, is about ~6x faster in testing and is more readable to boot.

# Complaints
Beyond the apparent desire to reinvent the standard library there are a number of complaints I had with this code. First the naming of `_all_chars` as it refers to a subset of all unicode characters, second `_line_break_chars` consists of two characters one of which doesn't break the line, third the null character regex matches on one single character. More generally we spend a lot of time here regenerating constants. The null character for instance is simply `\0`. Why are we computing `chr(0)`? All of this for nought as well given that a simple easy to read alternative is faster.

# What just happened now?
The original author of this code no longer works at my company which is a shame. I'd really like to talk to them to understand what was going through their head. I'm fairly certain that the developer was unaware of the import behavior and though that the regular expressions would be compiled once rather than on each function call should they be within the function `clean_string`. From that point of view this layout looks like an optimization. Still, why reinvent the standard library?

```
Thanks for reading
```
